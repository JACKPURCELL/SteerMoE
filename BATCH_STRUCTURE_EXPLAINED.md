# Batch Structure - æ‰¹æ¬¡ç»“æ„è¯¦è§£

## æ ¸å¿ƒæ”¹è¿›ï¼šæŒ‰ Batch ä¿å­˜æ•°æ®é›†

### é—®é¢˜èƒŒæ™¯

**ä¹‹å‰çš„é—®é¢˜**ï¼šè™½ç„¶æˆ‘ä»¬åœ¨åˆ†ææ—¶ä½¿ç”¨äº† batch å†…æ··åˆï¼ˆ8 safe + 8 unsafeï¼‰ï¼Œä½†è®­ç»ƒæ—¶æ•°æ®è¢«å±•å¹³å¹¶é‡æ–°æ‰“ä¹±ï¼Œå¯¼è‡´è®­ç»ƒå’Œåˆ†æä¸ä¸€è‡´ã€‚

```python
# åˆ†ææ—¶ï¼šBatch å†…æ··åˆ
Batch 0: [safe_0, safe_1, ..., safe_7, unsafe_0, ..., unsafe_7]  # 8+8=16
Batch 1: [safe_8, safe_9, ..., safe_15, unsafe_8, ..., unsafe_15]

# ä¹‹å‰è®­ç»ƒæ—¶ï¼šè¢«æ‰“ä¹±
Training Batch: [safe_0, unsafe_12, safe_25, ...]  # éšæœºç»„åˆï¼Œæ¯”ä¾‹ä¸å›ºå®š
```

### è§£å†³æ–¹æ¡ˆï¼šä¿æŒ Batch ç»“æ„

ç°åœ¨æ•°æ®é›†æŒ‰ç…§åˆ†ææ—¶çš„ batch ç»“æ„ä¿å­˜ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨ `BatchPreservingSampler` ç»´æŒè¿™ä¸ªç»“æ„ã€‚

```python
# åˆ†ææ—¶ï¼šBatch å†…æ··åˆ
Batch 0: [safe_0, ..., safe_7, unsafe_0, ..., unsafe_7]

# è®­ç»ƒæ—¶ï¼šä¿æŒç›¸åŒç»“æ„
Training Batch 0: [safe_0, ..., safe_7, unsafe_0, ..., unsafe_7]  # å®Œå…¨ç›¸åŒï¼
```

## å®ç°ç»†èŠ‚

### 1. æ•°æ®ä¿å­˜ï¼šæŒ‰ Batch ç»„ç»‡

**å‡½æ•°**: `create_batch_structured_dataset()`

```python
def create_batch_structured_dataset(sft_data, tokenizer, batch_size=16, safe_ratio=0.5):
    """
    è¿”å› List[List[Dict]]
    å¤–å±‚ List: æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª batch
    å†…å±‚ List: batch å†…çš„æ ·æœ¬ï¼Œå‰ 8 ä¸ª safeï¼Œå 8 ä¸ª unsafe
    """
    all_batches = []
    
    for batch_data in sft_data['batch_data_list']:
        batch_examples = []
        
        # å…ˆæ·»åŠ  safe samples
        for idx in batch_data['safe_indices']:
            batch_examples.append({
                "messages": [...],
                "label": "safe",
                "batch_idx": batch_data['batch_idx'],  # è®°å½• batch ID
                "in_batch_idx": len(batch_examples)     # è®°å½•åœ¨ batch å†…çš„ä½ç½®
            })
        
        # å†æ·»åŠ  unsafe samples
        for idx in batch_data['unsafe_indices']:
            batch_examples.append({
                "messages": [...],
                "label": "unsafe",
                "batch_idx": batch_data['batch_idx'],
                "in_batch_idx": len(batch_examples)
            })
        
        all_batches.append(batch_examples)  # ä¿å­˜æ•´ä¸ª batch
    
    return all_batches
```

**å…³é”®ç‚¹**ï¼š
- æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ `batch_idx` å­—æ®µï¼Œæ ‡è¯†å®ƒå±äºå“ªä¸ª batch
- æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ `in_batch_idx` å­—æ®µï¼Œæ ‡è¯†å®ƒåœ¨ batch å†…çš„ä½ç½®
- Batch å†…çš„é¡ºåºå›ºå®šï¼šå…ˆ safeï¼Œå unsafe

### 2. æ•°æ®åŠ è½½ï¼šè½¬æ¢ä¸º HuggingFace Dataset

**å‡½æ•°**: `create_batch_aware_dataset()`

```python
def create_batch_aware_dataset(batches, tokenizer, max_length=512):
    """
    å°† batch-structured æ•°æ®è½¬æ¢ä¸º HuggingFace Dataset
    ä½†ä¿ç•™ batch_idx ä¿¡æ¯
    """
    # å±•å¹³ä½†ä¿ç•™ batch ä¿¡æ¯
    all_examples = []
    for batch in batches:
        all_examples.extend(batch)
    
    # åˆ›å»º Datasetï¼ŒåŒ…å« batch_idx
    dataset_dict = {
        "messages": [ex["messages"] for ex in all_examples],
        "batch_idx": [ex["batch_idx"] for ex in all_examples],  # ä¿ç•™ï¼
        "label": [ex["label"] for ex in all_examples],
    }
    
    return Dataset.from_dict(dataset_dict)
```

**å…³é”®ç‚¹**ï¼š
- æ•°æ®è™½ç„¶å±•å¹³æˆä¸€ç»´åˆ—è¡¨ï¼Œä½†æ¯ä¸ªæ ·æœ¬ä¿ç•™äº† `batch_idx`
- Tokenize åï¼Œ`batch_idx` å­—æ®µä»ç„¶ä¿ç•™åœ¨ dataset ä¸­

### 3. è®­ç»ƒé‡‡æ ·ï¼šBatchPreservingSampler

**ç±»**: `BatchPreservingSampler`

```python
class BatchPreservingSampler(Sampler):
    def __init__(self, dataset, batch_size, shuffle_batches=True):
        # æŒ‰ batch_idx åˆ†ç»„
        self.batch_groups = {}
        for idx, item in enumerate(dataset):
            batch_idx = item['batch_idx']
            if batch_idx not in self.batch_groups:
                self.batch_groups[batch_idx] = []
            self.batch_groups[batch_idx].append(idx)
        
        # batch_groups[0] = [0, 1, 2, ..., 15]  # Batch 0 çš„æ ·æœ¬ç´¢å¼•
        # batch_groups[1] = [16, 17, 18, ..., 31]  # Batch 1 çš„æ ·æœ¬ç´¢å¼•
    
    def __iter__(self):
        # å¯ä»¥æ‰“ä¹± batch çš„é¡ºåº
        batch_ids = list(self.batch_groups.keys())
        if self.shuffle_batches:
            random.shuffle(batch_ids)
        
        # ä½† batch å†…çš„æ ·æœ¬é¡ºåºä¿æŒä¸å˜
        for batch_id in batch_ids:
            batch_indices = self.batch_groups[batch_id]
            yield from batch_indices  # æŒ‰åŸé¡ºåºè¾“å‡º
```

**å…³é”®ç‚¹**ï¼š
- å¯ä»¥æ‰“ä¹± batch çš„é¡ºåºï¼ˆepoch ä¹‹é—´å˜åŒ–ï¼‰
- ä½† batch å†…çš„æ ·æœ¬é¡ºåºå›ºå®šï¼ˆ8 safe + 8 unsafeï¼‰
- ç¡®ä¿æ¯ä¸ª training batch éƒ½æœ‰æ­£ç¡®çš„ safe/unsafe æ¯”ä¾‹

### 4. é›†æˆåˆ° Trainer

```python
# åˆ›å»º Trainer
trainer = ExpertMaskingTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # åŒ…å« batch_idx çš„ dataset
    tokenizer=tokenizer,
)

# æ›¿æ¢ DataLoaderï¼Œä½¿ç”¨è‡ªå®šä¹‰ Sampler
def get_batch_preserving_dataloader():
    dataloader = original_get_train_dataloader()
    return DataLoader(
        trainer.train_dataset,
        batch_size=batch_size,  # 16
        sampler=BatchPreservingSampler(  # ä½¿ç”¨è‡ªå®šä¹‰ samplerï¼
            trainer.train_dataset, 
            batch_size,
            shuffle_batches=True
        ),
        collate_fn=dataloader.collate_fn,
    )

trainer.get_train_dataloader = get_batch_preserving_dataloader
```

## å®Œæ•´æµç¨‹ç¤ºä¾‹

### åˆ†æé˜¶æ®µï¼ˆdemo_refactored.pyï¼‰

```python
# Step 1: å¤„ç†æ•°æ®ï¼Œç”Ÿæˆæ··åˆ batch
batch_data_list = process_mixed_batches(
    llm, tokenizer, 
    safe_prompts, unsafe_prompts,
    batch_size=16, safe_ratio=0.5
)

# batch_data_list[0] = {
#     "batch_idx": 0,
#     "safe_indices": [0, 1, 2, ..., 7],
#     "unsafe_indices": [8, 9, 10, ..., 15],
#     "safe_routings": [...],
#     "unsafe_routings": [...]
# }

# Step 2: åœ¨æ¯ä¸ª batch å†…è®¡ç®— risk_diff
for batch_data in batch_data_list:
    batch_risk_diff = calculate_risk_diff_per_batch(batch_data, num_experts_per_tok)

# Step 3: èšåˆæ‰€æœ‰ batch çš„ç»“æœ
risk_diff_df = aggregate_batch_risk_diffs(batch_risk_diffs)

# Step 4: ä¿å­˜ï¼ˆåŒ…å«å®Œæ•´çš„ batch ç»“æ„ï¼‰
sft_data = {
    "batch_data_list": batch_data_list,  # ä¿ç•™å®Œæ•´ batch ç»“æ„ï¼
    "unsafe_experts": unsafe_experts,
    "expert_mask": expert_mask,
}
pd.to_pickle(sft_data, "sft_dataset_*.pkl")
```

### è®­ç»ƒé˜¶æ®µï¼ˆtrain_batch_unsafe_experts.pyï¼‰

```python
# Step 1: åŠ è½½æ•°æ®ï¼ˆåŒ…å« batch ç»“æ„ï¼‰
sft_data = pd.read_pickle("sft_dataset_*.pkl")
batch_data_list = sft_data['batch_data_list']

# Step 2: åˆ›å»º batch-structured dataset
batches = create_batch_structured_dataset(sft_data, tokenizer)
# batches[0] = [
#     {"messages": ..., "label": "safe", "batch_idx": 0, "in_batch_idx": 0},
#     ...
#     {"messages": ..., "label": "safe", "batch_idx": 0, "in_batch_idx": 7},
#     {"messages": ..., "label": "unsafe", "batch_idx": 0, "in_batch_idx": 8},
#     ...
#     {"messages": ..., "label": "unsafe", "batch_idx": 0, "in_batch_idx": 15},
# ]

# Step 3: è½¬æ¢ä¸º HuggingFace Datasetï¼ˆä¿ç•™ batch_idxï¼‰
train_dataset = create_batch_aware_dataset(batches, tokenizer)

# Step 4: ä½¿ç”¨ BatchPreservingSampler è®­ç»ƒ
trainer = ExpertMaskingTrainer(...)
trainer.get_train_dataloader = get_batch_preserving_dataloader  # å…³é”®ï¼
trainer.train()
```

## éªŒè¯ Batch ç»“æ„

### æ£€æŸ¥æ•°æ®é›†

```python
import pandas as pd

# åŠ è½½æ•°æ®
sft_data = pd.read_pickle("sft_dataset_allenai--OLMoE-1B-7B-0125-Instruct.pkl")

# æŸ¥çœ‹ batch æ•°é‡
print(f"Number of batches: {len(sft_data['batch_data_list'])}")

# æŸ¥çœ‹ç¬¬ä¸€ä¸ª batch
batch_0 = sft_data['batch_data_list'][0]
print(f"Batch 0 safe indices: {batch_0['safe_indices']}")
print(f"Batch 0 unsafe indices: {batch_0['unsafe_indices']}")
print(f"Total samples in batch 0: {len(batch_0['batch_outputs'])}")
```

### æ£€æŸ¥è®­ç»ƒæ—¶çš„ batch

åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š

```python
# åœ¨è®­ç»ƒå‰éªŒè¯
sample_batch_indices = [i for i, item in enumerate(train_dataset) if item['batch_idx'] == 0]
print(f"Samples in batch 0: {len(sample_batch_indices)}")
print(f"Expected: {batch_size}")

# éªŒè¯ safe/unsafe åˆ†å¸ƒ
labels = [train_dataset[i]['label_str'] for i in sample_batch_indices]
print(f"Safe samples: {labels.count('safe')}")
print(f"Unsafe samples: {labels.count('unsafe')}")
```

### åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§

```python
class BatchMonitoringTrainer(ExpertMaskingTrainer):
    def training_step(self, model, inputs):
        # æ£€æŸ¥å½“å‰ batch çš„ batch_idx åˆ†å¸ƒ
        if 'batch_idx' in inputs:
            batch_ids = inputs['batch_idx'].cpu().numpy()
            unique_batches = np.unique(batch_ids)
            print(f"Current training batch contains samples from batches: {unique_batches}")
            # åº”è¯¥åªæœ‰ä¸€ä¸ª unique batch_idx
            assert len(unique_batches) == 1, "Batch structure broken!"
        
        return super().training_step(model, inputs)
```

## ä¼˜åŠ¿æ€»ç»“

### âœ… å®Œå…¨ä¸€è‡´æ€§

```
åˆ†æï¼šBatch 0 = [8 safe + 8 unsafe]
è®­ç»ƒï¼šBatch 0 = [8 safe + 8 unsafe]  â† å®Œå…¨ç›¸åŒï¼
```

### âœ… å¯é‡ç°æ€§

```
# ç›¸åŒçš„ batch ç»“æ„
# ç›¸åŒçš„ safe/unsafe æ¯”ä¾‹
# ç›¸åŒçš„ expert activation pattern
â†’ è®­ç»ƒç»“æœæ›´å¯é‡ç°
```

### âœ… å¯æ§æ€§

```python
# ç²¾ç¡®æ§åˆ¶æ¯ä¸ª batch çš„ç»„æˆ
Batch 0: 8 safe (easy) + 8 unsafe (hard)
Batch 1: 8 safe (medium) + 8 unsafe (extreme)
...
```

### âœ… å¯è¿½æº¯æ€§

```python
# è®­ç»ƒæ—¶å¯ä»¥è¿½æº¯åˆ°åˆ†ææ—¶çš„æ•°æ®
training_sample.batch_idx = 5
â†’ å¯¹åº” analysis batch 5
â†’ å¯ä»¥æŸ¥çœ‹è¯¥ batch çš„ risk_diff
â†’ å¯ä»¥åˆ†æä¸ºä»€ä¹ˆè¿™ä¸ª batch æ•ˆæœå¥½/å·®
```

## ä¸ä¹‹å‰å®ç°çš„å¯¹æ¯”

| æ–¹é¢ | ä¹‹å‰ï¼ˆå±•å¹³ï¼‰ | ç°åœ¨ï¼ˆä¿æŒç»“æ„ï¼‰ |
|------|------------|----------------|
| **æ•°æ®ç»„ç»‡** | å±•å¹³æˆä¸€ç»´åˆ—è¡¨ | æŒ‰ batch ç»„ç»‡ |
| **è®­ç»ƒæ—¶ batch** | éšæœºé‡ç»„ | ä¿æŒåŸå§‹ç»“æ„ |
| **Safe/Unsafe æ¯”ä¾‹** | æ¯ä¸ª batch ä¸å›ºå®š | æ¯ä¸ª batch å›ºå®š 50/50 |
| **åˆ†æä¸è®­ç»ƒä¸€è‡´æ€§** | âŒ ä¸ä¸€è‡´ | âœ… å®Œå…¨ä¸€è‡´ |
| **å¯è¿½æº¯æ€§** | âŒ éš¾ä»¥è¿½æº¯ | âœ… å®Œå…¨å¯è¿½æº¯ |
| **å®éªŒé‡ç°æ€§** | âš ï¸ è¾ƒå·® | âœ… ä¼˜ç§€ |

## æŠ€æœ¯è¦ç‚¹

### 1. ä¿ç•™ batch_idx æ˜¯å…³é”®

```python
# åœ¨æ¯ä¸ªé˜¶æ®µéƒ½ä¿ç•™ batch_idx
create_batch_structured_dataset  â†’ æ·»åŠ  batch_idx
create_batch_aware_dataset       â†’ ä¿ç•™ batch_idx (åœ¨ tokenize ä¸­)
BatchPreservingSampler           â†’ ä½¿ç”¨ batch_idx åˆ†ç»„
```

### 2. ä¸åœ¨ batch å†…æ‰“ä¹±

```python
# âœ… æ­£ç¡®ï¼šå¯ä»¥æ‰“ä¹± batch é¡ºåº
batch_order = [5, 2, 8, 1, ...]  # random order

# âœ… æ­£ç¡®ï¼šbatch å†…é¡ºåºä¸å˜
Batch 5: [safe_0, ..., safe_7, unsafe_0, ..., unsafe_7]  # å›ºå®šé¡ºåº

# âŒ é”™è¯¯ï¼šæ‰“ä¹± batch å†…é¡ºåº
Batch 5: [unsafe_3, safe_1, unsafe_0, ...]  # è¿™æ ·ä¼šç ´åç»“æ„
```

### 3. DataLoader é…ç½®

```python
TrainingArguments(
    per_device_train_batch_size=16,  # å¿…é¡»ä¸åˆ†ææ—¶ç›¸åŒ
    dataloader_drop_last=False,      # ä¸ä¸¢å¼ƒä¸å®Œæ•´ batch
    group_by_length=False,           # ä¸æŒ‰é•¿åº¦åˆ†ç»„
    # shuffle é€šè¿‡ sampler æ§åˆ¶
)
```

## æ€»ç»“

é€šè¿‡æŒ‰ batch ä¿å­˜æ•°æ®é›†å’Œä½¿ç”¨ `BatchPreservingSampler`ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š

1. **åˆ†æä¸è®­ç»ƒå®Œå…¨ä¸€è‡´**ï¼šç›¸åŒçš„ batch ç»“æ„ï¼Œç›¸åŒçš„ safe/unsafe æ¯”ä¾‹
2. **æ›´å¯é çš„å®éªŒç»“æœ**ï¼šå‡å°‘äº†éšæœºæ€§ï¼Œæé«˜äº†å¯é‡ç°æ€§
3. **æ›´å¥½çš„å¯è¿½æº¯æ€§**ï¼šå¯ä»¥ç²¾ç¡®è¿½è¸ªæ¯ä¸ªè®­ç»ƒæ ·æœ¬åˆ°åˆ†æé˜¶æ®µ
4. **æ›´å®¹æ˜“è°ƒè¯•**ï¼šå¯ä»¥é’ˆå¯¹ç‰¹å®š batch è¿›è¡Œåˆ†æå’Œä¼˜åŒ–

è¿™æ˜¯å®ç°çœŸæ­£çš„ batch å†…è®­ç»ƒçš„å…³é”®ï¼ğŸ¯

