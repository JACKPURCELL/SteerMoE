{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558b770a",
   "metadata": {},
   "source": [
    "# Steering MoE LLMs with Custom Datasets\n",
    "\n",
    "In this tutorial, we show how to steer MoE LLMs using a custom dataset.\n",
    "We focus on controlling whether the model outputs digits (1, 2, 3) or written numbers (one, two, three), and we can identify the relevant experts with just one example pair!\n",
    "\n",
    "Steps:\n",
    "1. Load a pre-trained MoE LLM.\n",
    "\n",
    "2. Prepare a custom steering dataset.\n",
    "\n",
    "3. Save routing activations on the dataset pairs.\n",
    "\n",
    "4. Identify behavior-linked experts via risk difference.\n",
    "\n",
    "5. (De)activate experts at inference to steer model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac9f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:12:53 [__init__.py:241] Automatically detected platform cuda.\n",
      "HF_TOKEN not found in environment variables. Continuing without login.\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2022 Adobe\n",
    "# All Rights Reserved.\n",
    "\n",
    "# NOTICE: Adobe permits you to use, modify, and distribute this file in\n",
    "# accordance with the terms of the Adobe license agreement accompanying\n",
    "# it.\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/localssd/.hfcache/\"\n",
    "os.environ[\"VLLM_ALLOW_INSECURE_SERIALIZATION\"] = \"1\"\n",
    "os.environ[\"VLLM_DISABLE_COMPILE_CACHE\"] = \"1\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"true\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "os.environ[\"TEMP_NPY_BASE_PATH\"] = \"./temp_routings/\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from importlib import reload\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub as hf_hub\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.utils import register_vllm_save_models, register_vllm_models, steer_moe\n",
    "\n",
    "try:\n",
    "    load_dotenv()\n",
    "    hf_hub.login(os.environ[\"HF_TOKEN\"])\n",
    "except Exception as e:\n",
    "    print(\"HF_TOKEN not found in environment variables. Continuing without login.\")\n",
    "    pass\n",
    "\n",
    "if not os.path.exists(os.environ[\"TEMP_NPY_BASE_PATH\"]):\n",
    "    os.makedirs(os.environ[\"TEMP_NPY_BASE_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db478e5",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835003bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.qwen3_moe:Qwen3MoeForCausalLM.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture MixtralForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.mixtral:MixtralForCausalLM.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture OlmoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.olmoe:OlmoeForCausalLM.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.deepseek_v2:DeepseekV2ForCausalLM.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture Llama4ForConditionalGeneration is already registered, and will be overwritten by the new model class src.modeling_vllm_save.mllama4:Llama4ForConditionalGeneration.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture GptOssForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.gpt_oss:GptOssForCausalLM.\n",
      "WARNING 09-02 18:12:54 [registry.py:458] Model architecture PhiMoEForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm_save.phimoe:PhiMoEForCausalLM.\n",
      "INFO 09-02 18:12:54 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-30B-A3B', 'trust_remote_code': True, 'max_model_len': 4000, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 1, 'disable_log_stats': True, 'enforce_eager': True, 'max_seq_len_to_capture': 4000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:13:02 [__init__.py:711] Resolved architecture: Qwen3MoeForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:13:02 [__init__.py:1750] Using max model len 4000\n",
      "INFO 09-02 18:13:05 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 09-02 18:13:05 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4000). This may lead to unexpected behavior.\n",
      "WARNING 09-02 18:13:05 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4000). This may lead to unexpected behavior.\n",
      "INFO 09-02 18:13:05 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "WARNING 09-02 18:13:06 [serial_utils.py:48] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:06 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:06 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-30B-A3B', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:09 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m WARNING 09-02 18:13:09 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:09 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:09 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:09 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=0\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=1\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=2\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=3\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=4\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=5\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=6\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=7\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=8\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=9\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=10\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=11\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=12\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=13\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=14\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=15\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=16\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=17\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=18\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=19\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=20\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=21\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=22\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=23\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=24\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=25\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=26\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=27\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=28\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=29\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=30\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=31\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=32\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=33\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=34\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=35\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=36\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=37\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=38\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=39\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=40\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=41\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=42\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=43\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=44\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=45\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=46\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m update: layer_idx=47\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:10 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314fbc5db30c408ca058002848447635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:29 [default_loader.py:262] Loading weights took 19.11 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:30 [gpu_model_runner.py:2007] Model loading took 56.8814 GiB and 20.010527 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m WARNING 09-02 18:13:31 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/colligo/anaconda3/envs/steermoe-env/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_A100-SXM4-80GB.json']\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:37 [gpu_worker.py:276] Available KV cache memory: 17.83 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:38 [kv_cache_utils.py:849] GPU KV cache size: 194,784 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:38 [kv_cache_utils.py:853] Maximum concurrency for 4,000 tokens per request: 48.70x\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:38 [core.py:214] init engine (profile, create kv cache, warmup model) took 8.06 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m INFO 09-02 18:13:39 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "INFO 09-02 18:13:39 [llm.py:298] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3336907)\u001b[0;0m WARNING 09-02 18:13:41 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    }
   ],
   "source": [
    "# Supported Models: \n",
    "# \"Qwen/Qwen3-30B-A3B\", \"openai/gpt-oss-120b\", \n",
    "# \"microsoft/Phi-3.5-MoE-instruct\", \"openai/gpt-oss-20b\", \n",
    "# \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"allenai/OLMoE-1B-7B-0125-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B\"\n",
    "\n",
    "register_vllm_save_models()\n",
    "sampling_params = SamplingParams(temperature=0, top_p=0.8, top_k=1, min_p=0, max_tokens=1, seed=0)\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME, \n",
    "    max_seq_len_to_capture=4000, max_model_len=4000, \n",
    "    tensor_parallel_size=torch.cuda.device_count(), gpu_memory_utilization=0.95, max_num_seqs=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c4887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:13:41 [chat_utils.py:470] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "dict_keys(['router_logits', 'messages', 'prompt_token_ids']) 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((48, 13, 128),\n",
       " array([[[-4.062, -3.86 ],\n",
       "         [-4.22 , -5.594]],\n",
       " \n",
       "        [[-3.188, -4.562],\n",
       "         [-4.97 , -4.688]]], dtype=float16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_routings(messages):\n",
    "    \"\"\"\n",
    "    Get the routing logits for the given messages.\n",
    "    \"\"\"\n",
    "    for layer in range(500):\n",
    "        TEMP_NPY_PATH = f\"{os.environ['TEMP_NPY_BASE_PATH']}/router_logits_L{layer}.npy\"\n",
    "        if os.path.exists(TEMP_NPY_PATH):\n",
    "            os.remove(TEMP_NPY_PATH)\n",
    "        \n",
    "    outputs = llm.chat(messages, sampling_params, use_tqdm=False, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"})\n",
    "    \n",
    "    all_router_logits = []\n",
    "    for layer in range(500):\n",
    "        try:\n",
    "            TEMP_NPY_PATH = f\"{os.environ['TEMP_NPY_BASE_PATH']}/router_logits_L{layer}.npy\"\n",
    "            router_logits = np.load(TEMP_NPY_PATH).astype(np.float16)\n",
    "            all_router_logits.append(router_logits)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    all_router_logits = np.stack(all_router_logits, axis=0)  # (num_layers, num_tokens, n_experts)\n",
    "    output = {\n",
    "        \"router_logits\": all_router_logits.astype(np.float16),  # (num_layers, num_tokens, n_experts)\n",
    "        \"messages\": messages,\n",
    "        \"prompt_token_ids\": outputs[0].prompt_token_ids,\n",
    "    }\n",
    "    return output\n",
    "\n",
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"Hello\"},]\n",
    "]\n",
    "r = get_routings(messages)\n",
    "print(r.keys(), len(r[\"prompt_token_ids\"]))\n",
    "r[\"router_logits\"].shape, r[\"router_logits\"][:2, :2, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f45f0",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce3526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages_0</th>\n",
       "      <th>messages_1</th>\n",
       "      <th>messages_0_target</th>\n",
       "      <th>messages_1_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'user', 'content': 'Count to ten'}, ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Count to ten'}, ...</td>\n",
       "      <td>1, 2, 3, 4, 5, 6, 7, 8, 9, 10</td>\n",
       "      <td>one, two, three, four, five, six, seven, eight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          messages_0  \\\n",
       "0  [{'role': 'user', 'content': 'Count to ten'}, ...   \n",
       "\n",
       "                                          messages_1  \\\n",
       "0  [{'role': 'user', 'content': 'Count to ten'}, ...   \n",
       "\n",
       "               messages_0_target  \\\n",
       "0  1, 2, 3, 4, 5, 6, 7, 8, 9, 10   \n",
       "\n",
       "                                   messages_1_target  \n",
       "0  one, two, three, four, five, six, seven, eight...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### - messages_0: The messages for the first behavior response (ex safe)\n",
    "### - messages_1: The messages for the second behavior response (ex unsafe)\n",
    "### - messages_0_target: The target string for the first behavior response (Which tokens to compare routings)\n",
    "### - messages_1_target: The target string for the second behavior response (Which tokens to compare routings)\n",
    "DATASET_NAME = \"custom_dataset\"\n",
    "\n",
    "df_ds = pd.DataFrame([\n",
    "    {\n",
    "        \"messages_0\": [{\"role\": \"user\", \"content\": \"Count to ten\"}, {\"role\": \"assistant\", \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"}],\n",
    "        \"messages_1\": [{\"role\": \"user\", \"content\": \"Count to ten\"}, {\"role\": \"assistant\", \"content\": \"one, two, three, four, five, six, seven, eight, nine, ten\"}],\n",
    "        \"messages_0_target\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\",\n",
    "        \"messages_1_target\": \"one, two, three, four, five, six, seven, eight, nine, ten\",\n",
    "    },\n",
    "])\n",
    "\n",
    "df_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d55d9e",
   "metadata": {},
   "source": [
    "# 3. Save Routings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e102dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65198b50a346441f8febee398a1f3e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 53, 128)\n",
      "8\n",
      "dict_keys(['router_logits', 'messages', 'prompt_token_ids', 'prompt_tokens', 'prompt_tokens_special_mask', 'messages_tokenized', 'messages_0_target'])\n",
      "### SAVED ROUTINGS AT: output_[Qwen--Qwen3-30B-A3B]_[custom_dataset]_[messages_0]_[1].pkl\n",
      "1\n",
      "{'model_name': 'Qwen/Qwen3-30B-A3B', 'dataset_name': 'custom_dataset', 'doc_choice': 'messages_0', 'num_experts': 128, 'num_experts_per_tok': '8', 'col_names': {}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaa052f6c58449ba87a0cb91b040ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 43, 128)\n",
      "8\n",
      "dict_keys(['router_logits', 'messages', 'prompt_token_ids', 'prompt_tokens', 'prompt_tokens_special_mask', 'messages_tokenized', 'messages_1_target'])\n",
      "### SAVED ROUTINGS AT: output_[Qwen--Qwen3-30B-A3B]_[custom_dataset]_[messages_1]_[1].pkl\n",
      "1\n",
      "{'model_name': 'Qwen/Qwen3-30B-A3B', 'dataset_name': 'custom_dataset', 'doc_choice': 'messages_1', 'num_experts': 128, 'num_experts_per_tok': '8', 'col_names': {}}\n"
     ]
    }
   ],
   "source": [
    "for PAIR_CHOICE in [\"messages_0\", \"messages_1\"]:\n",
    "    def find_sub_list(sl,l):\n",
    "        results = []\n",
    "        sll = len(sl)\n",
    "        for ind in (i for i,e in enumerate(l) if e == sl[0]):\n",
    "            if l[ind:ind+sll] == sl:\n",
    "                results.append((ind, ind + sll - 1))\n",
    "        return results\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    outputs_list = []\n",
    "    for i in tqdm(range(len(df_ds))):\n",
    "        # Get routings\n",
    "        messages = df_ds.iloc[i][PAIR_CHOICE]\n",
    "        outputs = get_routings(messages)  # 'router_logits', 'messages', 'prompt_token_ids'\n",
    "        # Check shapes\n",
    "        num_layers, num_tokens, n_experts = outputs[\"router_logits\"].shape\n",
    "        assert num_tokens == len(outputs[\"prompt_token_ids\"])\n",
    "        print(outputs[\"router_logits\"].shape)\n",
    "        # Store prompt tokens\n",
    "        outputs[\"prompt_tokens\"] = tokenizer.convert_ids_to_tokens(outputs[\"prompt_token_ids\"], skip_special_tokens=False)\n",
    "        outputs[\"prompt_tokens_special_mask\"] = tokenizer.get_special_tokens_mask(\n",
    "            outputs[\"prompt_token_ids\"], already_has_special_tokens=True,\n",
    "        )\n",
    "        # Store the messages in a tokenized format\n",
    "        outputs[\"messages_tokenized\"] = [{\n",
    "            \"role\": message[\"role\"], \n",
    "            \"content_token_ids\": tokenizer(message[\"content\"], add_special_tokens=False).input_ids,\n",
    "            \"content_tokens\": tokenizer.convert_ids_to_tokens(tokenizer(message[\"content\"], add_special_tokens=False).input_ids)\n",
    "        } for message in messages]\n",
    "        # Store the target texts and their tokenized forms (for detection comparison on these tokens)\n",
    "        for col in [f\"{PAIR_CHOICE}_target\"]:\n",
    "            target_text = df_ds.iloc[i][col]\n",
    "            target_token_ids = tokenizer(df_ds.iloc[i][col], add_special_tokens=False).input_ids\n",
    "            target_tokens = tokenizer.convert_ids_to_tokens(target_token_ids, skip_special_tokens=False)\n",
    "            locations = find_sub_list(target_token_ids, outputs[\"prompt_token_ids\"])\n",
    "            assert len(locations) >= 1, f\"Expected exactly one location: {locations}, for target text: \\n{target_tokens}, in prompt tokens: \\n{outputs['prompt_tokens']}\"\n",
    "            if len(locations) > 1:\n",
    "                print(f\"Expected exactly one location: {locations}, for target text: \\n{target_tokens}, in prompt tokens: \\n{outputs['prompt_tokens']}\")\n",
    "                print(\"Using last one\")\n",
    "                locations[0] = locations[-1]  # Use the last location if there are multiple\n",
    "            outputs[col] = {\n",
    "                \"text\": target_text,\n",
    "                \"tokens\": target_tokens,\n",
    "                \"token_ids\": target_token_ids,\n",
    "                \"start_idx\": locations[0][0] if locations else None,\n",
    "                \"end_idx\": locations[0][1] if locations else None,\n",
    "            }\n",
    "        # Append the outputs for this example to the list\n",
    "        outputs_list.append(outputs)\n",
    "\n",
    "    def get_model_num_experts(self):\n",
    "        model = self.model_runner.model\n",
    "        if hasattr(model, \"model_config\") and hasattr(model.model_config, \"num_experts_per_tok\"):\n",
    "            return model.model_config.num_experts_per_tok  # gpt-oss\n",
    "        elif hasattr(model.config, \"num_experts_per_tok\"):\n",
    "            num_experts_per_tok = f\"{model.config.num_experts_per_tok}\"\n",
    "        else:\n",
    "            num_experts_per_tok = f\"{model.config.text_config.num_experts_per_tok}\"  # llama4\n",
    "        return num_experts_per_tok\n",
    "    num_experts_per_tok = llm.collective_rpc(get_model_num_experts)[0]\n",
    "    print(num_experts_per_tok)\n",
    "    print(outputs_list[0].keys())\n",
    "    df = pd.DataFrame(outputs_list)\n",
    "    df.attrs = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset_name\": DATASET_NAME,\n",
    "        \"doc_choice\": PAIR_CHOICE,\n",
    "        \"num_experts\": n_experts,\n",
    "        \"num_experts_per_tok\": num_experts_per_tok,\n",
    "        \"col_names\": {},\n",
    "    }\n",
    "    path = f\"output_[{MODEL_NAME.replace('/', '--')}]_[{DATASET_NAME}]_[{PAIR_CHOICE}]_[{len(df)}].pkl\"\n",
    "    df.to_pickle(path)\n",
    "    print(\"### SAVED ROUTINGS AT:\", path)\n",
    "    print(len(df))\n",
    "    print(df.attrs)\n",
    "    df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669a37f",
   "metadata": {},
   "source": [
    "# 4. Detect Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b68cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    \"messages_0\": pd.read_pickle(f\"output_[{MODEL_NAME.replace('/', '--')}]_[{DATASET_NAME}]_[messages_0]_[{len(df)}].pkl\"),\n",
    "    \"messages_1\": pd.read_pickle(f\"output_[{MODEL_NAME.replace('/', '--')}]_[{DATASET_NAME}]_[messages_1]_[{len(df)}].pkl\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c0e60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853c261f826a4d8f800e4f3778f3d2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_df_1: messages_0, key_df_2: messages_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4449b5d352e144248d619c81e0732ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "(29, 48, 128) (19, 48, 128)\n",
      "Used examples: 1\n"
     ]
    }
   ],
   "source": [
    "TOKEN_REDUCE_FN = \"rd\"\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind:ind+sll] == sl:\n",
    "            results.append((ind, ind + sll - 1))\n",
    "    return results  # [start_idx, end_idx]\n",
    "\n",
    "def get_router_prob_n2(row):\n",
    "    \"\"\"Get the probability of each expert selected by the router for a given token.\"\"\"\n",
    "    router_logits = torch.tensor(row[\"router_logits\"])  # (layer, token, expert)\n",
    "    router_prob = torch.nn.functional.softmax(router_logits, dim=-1)  # (layer, token, expert)\n",
    "    return router_prob.cpu().numpy()  # (layer, token, expert)\n",
    "\n",
    "for key in tqdm(dfs.keys()):\n",
    "    dfs[key][\"router_prob_n2\"] = dfs[key].apply(get_router_prob_n2, axis=1)  # (layer, token, expert)\n",
    "\n",
    "# Concat router freq for all examples\n",
    "key_df_1 = \"messages_0\"\n",
    "key_df_2 = \"messages_1\"\n",
    "print(f\"key_df_1: {key_df_1}, key_df_2: {key_df_2}\")\n",
    "freq = {key_df_1: [], key_df_2: []}\n",
    "tokens = {key_df_1: [], key_df_2: []}\n",
    "\n",
    "debug_example_starts = []\n",
    "num_used_examples = 0\n",
    "for row_idx in tqdm(range(0, len(dfs[key_df_1]))):\n",
    "    router_prob_n2_1 = dfs[key_df_1].iloc[row_idx][\"router_prob_n2\"]  # (layer, token, expert)\n",
    "    router_prob_n2_2 = dfs[key_df_2].iloc[row_idx][\"router_prob_n2\"]  # (layer, token, expert)\n",
    "    num_tokens_1, num_tokens_2 = router_prob_n2_1.shape[1], router_prob_n2_2.shape[1]\n",
    "\n",
    "    subset_1 = dfs[key_df_1].iloc[row_idx][\"messages_0_target\"][\"token_ids\"]\n",
    "    subset_2 = dfs[key_df_2].iloc[row_idx][\"messages_1_target\"][\"token_ids\"]\n",
    "    range_1 = find_sub_list(subset_1, dfs[key_df_1].iloc[row_idx][\"prompt_token_ids\"])\n",
    "    range_2 = find_sub_list(subset_2, dfs[key_df_2].iloc[row_idx][\"prompt_token_ids\"])\n",
    "    assert len(range_1) >= 1 and len(range_2) >= 1, f\"Expected more than one range for each dataset, got {len(range_1)} and {len(range_2)}\"\n",
    "    range_1 = range_1[-1]\n",
    "    range_2 = range_2[-1]\n",
    "    num_used_examples += 1\n",
    "    debug_example_starts.append(len(freq[key_df_1]))\n",
    "\n",
    "    for token_1_idx in range(range_1[0], range_1[1] + 1):\n",
    "        freq[key_df_1].append(router_prob_n2_1[:, token_1_idx, :])\n",
    "        tokens[key_df_1].append(dfs[key_df_1].iloc[row_idx]['prompt_tokens'][token_1_idx])\n",
    "\n",
    "    for token_2_idx in range(range_2[0], range_2[1] + 1):\n",
    "        freq[key_df_2].append(router_prob_n2_2[:, token_2_idx, :])\n",
    "        tokens[key_df_2].append(dfs[key_df_2].iloc[row_idx]['prompt_tokens'][token_2_idx])\n",
    "    \n",
    "    if len(freq[key_df_1]) > 2000000:\n",
    "        print(\"Reached 2M token comparisons, stopping...\")\n",
    "        break\n",
    "\n",
    "print(len(freq[key_df_1]))\n",
    "freq[key_df_1] = np.stack(freq[key_df_1])\n",
    "freq[key_df_2] = np.stack(freq[key_df_2])\n",
    "print(freq[key_df_1].shape, freq[key_df_2].shape)\n",
    "print(f\"Used examples: {num_used_examples}\")\n",
    "\n",
    "if \"eq\" in TOKEN_REDUCE_FN:\n",
    "    # Equalize the number of tokens in both datasets\n",
    "    min_tokens = min(len(freq[key_df_1]), len(freq[key_df_2]))\n",
    "    freq[key_df_1] = freq[key_df_1][:min_tokens]\n",
    "    freq[key_df_2] = freq[key_df_2][:min_tokens]\n",
    "    tokens[key_df_1] = tokens[key_df_1][:min_tokens]\n",
    "    tokens[key_df_2] = tokens[key_df_2][:min_tokens]\n",
    "    print(freq[key_df_1].shape, freq[key_df_2].shape)\n",
    "# dfs['safe'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec5bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experts per token: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c6b7e2dc2b4028b7e9f04c3c2dc853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d54220820046faa6764748f8870b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a4e8548899402fb6a93c471c66133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to activations_[Qwen--Qwen3-30B-A3B]_[custom_dataset]_[rd]_[1]_[29].pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>expert</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>a1_n</th>\n",
       "      <th>a2_n</th>\n",
       "      <th>risk_diff</th>\n",
       "      <th>Layer_Expert</th>\n",
       "      <th>risk_diff_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>-0.878403</td>\n",
       "      <td>L45\\nE20</td>\n",
       "      <td>0.878403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>41</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>L45\\nE41</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.758621</td>\n",
       "      <td>L05\\nE22</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>-0.705989</td>\n",
       "      <td>L42\\nE30</td>\n",
       "      <td>0.705989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>115</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>-0.705989</td>\n",
       "      <td>L40\\nE115</td>\n",
       "      <td>0.705989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6139</th>\n",
       "      <td>18</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L18\\nE107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6140</th>\n",
       "      <td>18</td>\n",
       "      <td>106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L18\\nE106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6141</th>\n",
       "      <td>18</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L18\\nE105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L18\\nE104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>18</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L18\\nE114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6144 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      layer  expert    a1    a2    d1    d2      a1_n      a2_n  risk_diff  \\\n",
       "0        45      20   2.0  18.0  27.0   1.0  0.068966  0.947368  -0.878403   \n",
       "1        45      41  24.0   0.0   5.0  19.0  0.827586  0.000000   0.827586   \n",
       "2         5      22   7.0  19.0  22.0   0.0  0.241379  1.000000  -0.758621   \n",
       "3        42      30   7.0  18.0  22.0   1.0  0.241379  0.947368  -0.705989   \n",
       "4        40     115   7.0  18.0  22.0   1.0  0.241379  0.947368  -0.705989   \n",
       "...     ...     ...   ...   ...   ...   ...       ...       ...        ...   \n",
       "6139     18     107   0.0   0.0  29.0  19.0  0.000000  0.000000   0.000000   \n",
       "6140     18     106   0.0   0.0  29.0  19.0  0.000000  0.000000   0.000000   \n",
       "6141     18     105   0.0   0.0  29.0  19.0  0.000000  0.000000   0.000000   \n",
       "6142     18     104   0.0   0.0  29.0  19.0  0.000000  0.000000   0.000000   \n",
       "6143     18     114   0.0   0.0  29.0  19.0  0.000000  0.000000   0.000000   \n",
       "\n",
       "     Layer_Expert  risk_diff_abs  \n",
       "0        L45\\nE20       0.878403  \n",
       "1        L45\\nE41       0.827586  \n",
       "2        L05\\nE22       0.758621  \n",
       "3        L42\\nE30       0.705989  \n",
       "4       L40\\nE115       0.705989  \n",
       "...           ...            ...  \n",
       "6139    L18\\nE107       0.000000  \n",
       "6140    L18\\nE106       0.000000  \n",
       "6141    L18\\nE105       0.000000  \n",
       "6142    L18\\nE104       0.000000  \n",
       "6143    L18\\nE114       0.000000  \n",
       "\n",
       "[6144 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "NUM_EXPERTS_PER_TOK = int(dfs[key_df_1].attrs[\"num_experts_per_tok\"])\n",
    "print(f\"Number of experts per token: {NUM_EXPERTS_PER_TOK}\")\n",
    "\n",
    "def calc_risk_diff(prob1, prob2):\n",
    "    ### prob1,2 = (batch, layer, expert)\n",
    "    ### Count how many times each expert is activated\n",
    "    a1, a2, d1, d2 = np.zeros((prob1.shape[1], prob1.shape[2])), np.zeros((prob2.shape[1], prob2.shape[2])), np.zeros((prob1.shape[1], prob1.shape[2])), np.zeros((prob2.shape[1], prob2.shape[2]))\n",
    "    pre_processed_act1 = np.argsort(prob1, axis=-1)  # Get top experts\n",
    "    pre_processed_act2 = np.argsort(prob2, axis=-1)  # Get top experts\n",
    "    \n",
    "    for token_idx in tqdm(range(prob1.shape[0])):\n",
    "        for layer in range(prob1.shape[1]):\n",
    "            activated_experts_1 = pre_processed_act1[token_idx, layer, -NUM_EXPERTS_PER_TOK:]  # Get top 8 experts\n",
    "            a1[layer, activated_experts_1] += 1\n",
    "            deactivated_experts_1 = pre_processed_act1[token_idx, layer, :-NUM_EXPERTS_PER_TOK]  # Experts not activated in prob1\n",
    "            d1[layer, deactivated_experts_1] += 1\n",
    "            assert len(activated_experts_1) + len(deactivated_experts_1) == prob1.shape[2]  # num experts\n",
    "    \n",
    "    for token_idx in tqdm(range(prob2.shape[0])):\n",
    "        for layer in range(prob2.shape[1]):\n",
    "            activated_experts_2 = pre_processed_act2[token_idx, layer, -NUM_EXPERTS_PER_TOK:]\n",
    "            a2[layer, activated_experts_2] += 1\n",
    "            deactivated_experts_2 = pre_processed_act2[token_idx, layer, :-NUM_EXPERTS_PER_TOK]  # Experts not activated in prob2\n",
    "            d2[layer, deactivated_experts_2] += 1\n",
    "            assert len(activated_experts_2) + len(deactivated_experts_2) == prob2.shape[2]  # num experts\n",
    "\n",
    "    layer_expert_paired_ttest = []\n",
    "    for layer in tqdm(range(prob1.shape[1])):\n",
    "        for expert in range(prob1.shape[2]):\n",
    "            test_results = {\n",
    "                \"layer\": layer,\n",
    "                \"expert\": expert,\n",
    "                \"a1\": a1[layer, expert],\n",
    "                \"a2\": a2[layer, expert],\n",
    "                \"d1\": d1[layer, expert],\n",
    "                \"d2\": d2[layer, expert],\n",
    "                \"a1_n\": (a1[layer, expert] / (a1[layer, expert] + d1[layer, expert])),\n",
    "                \"a2_n\": (a2[layer, expert] / (a2[layer, expert] + d2[layer, expert])),\n",
    "                \"risk_diff\": (a1[layer, expert] / (a1[layer, expert] + d1[layer, expert])) - (a2[layer, expert] / (a2[layer, expert] + d2[layer, expert]))\n",
    "            }\n",
    "            layer_expert_paired_ttest.append(test_results)\n",
    "    return pd.DataFrame(layer_expert_paired_ttest)\n",
    "\n",
    "\n",
    "subset1, subset2 = \"messages_0\", \"messages_1\"  # \"random_doc\"\n",
    "df = calc_risk_diff(freq[subset1], freq[subset2])\n",
    "df[\"Layer_Expert\"] = df.apply(lambda x: f\"L{int(x['layer']):02d}\\nE{int(x['expert']):02d}\", axis=1)\n",
    "\n",
    "df[\"risk_diff_abs\"] = df[\"risk_diff\"].abs()\n",
    "df = df.sort_values(by=\"risk_diff_abs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "path = f\"activations_[{dfs[subset1].attrs['model_name'].replace('/', '--')}]_[{dfs[subset1].attrs['dataset_name']}]_[{TOKEN_REDUCE_FN}]_[{len(dfs[key_df_1])}]_[{len(freq[subset1])}].pkl\"\n",
    "df.to_pickle(path)\n",
    "print(f\"Saved to {path}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26429fc3",
   "metadata": {},
   "source": [
    "# 5. Steer MoE LLM using Detected Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38b8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESTART HERE TO FREE GPU MEMORY FOR THE NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10d7cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:13:59 [__init__.py:241] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/localssd/.hfcache/\"\n",
    "os.environ[\"VLLM_ALLOW_INSECURE_SERIALIZATION\"] = \"1\"\n",
    "os.environ[\"VLLM_DISABLE_COMPILE_CACHE\"] = \"1\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"true\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from importlib import reload\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub as hf_hub\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from src.utils import register_vllm_models, steer_moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ee2990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Qwen/Qwen3-30B-A3B',\n",
       " 'task': 'custom_dataset',\n",
       " 'max_tokens': 64,\n",
       " 'activations_path': 'activations_[Qwen--Qwen3-30B-A3B]_[custom_dataset]_[rd]_[1]_[29].pkl',\n",
       " 'num_pos_experts': 0,\n",
       " 'num_neg_experts': 250}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"Qwen/Qwen3-30B-A3B\"\n",
    "DATASET = \"custom_dataset\"\n",
    "config = {\n",
    "    \"model\": MODEL,\n",
    "    \"task\": DATASET,\n",
    "    \"max_tokens\": 64,\n",
    "\n",
    "    \"activations_path\": f\"activations_[{MODEL.replace('/', '--')}]_[{DATASET}]_[rd]_[1]_[29].pkl\",\n",
    "    \"num_pos_experts\": 0,  # Adjust these based on the model and task\n",
    "    \"num_neg_experts\": 250,  # Adjust these based on the model and task\n",
    "}\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5591f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.qwen3_moe:Qwen3MoeForCausalLM.\n",
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture MixtralForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.mixtral:MixtralForCausalLM.\n",
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture OlmoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.olmoe:OlmoeForCausalLM.\n",
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture Llama4ForConditionalGeneration is already registered, and will be overwritten by the new model class src.modeling_vllm.mllama4:Llama4ForConditionalGeneration.\n",
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture GptOssForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.gpt_oss:GptOssForCausalLM.\n",
      "WARNING 09-02 18:14:00 [registry.py:458] Model architecture PhiMoEForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.phimoe:PhiMoEForCausalLM.\n",
      "INFO 09-02 18:14:00 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-30B-A3B', 'trust_remote_code': True, 'max_model_len': 4096, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 1, 'disable_log_stats': True, 'enforce_eager': True, 'max_seq_len_to_capture': 4096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:14:08 [__init__.py:711] Resolved architecture: Qwen3MoeForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 18:14:08 [__init__.py:1750] Using max model len 4096\n",
      "INFO 09-02 18:14:10 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 09-02 18:14:10 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4096). This may lead to unexpected behavior.\n",
      "WARNING 09-02 18:14:10 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4096). This may lead to unexpected behavior.\n",
      "INFO 09-02 18:14:10 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "WARNING 09-02 18:14:12 [serial_utils.py:48] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:12 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:12 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-30B-A3B', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:14 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m WARNING 09-02 18:14:14 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:14 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:15 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:15 [cuda.py:328] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921e0d9584bc4a95be162c8dbc9b767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:32 [default_loader.py:262] Loading weights took 15.27 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:33 [gpu_model_runner.py:2007] Model loading took 56.8820 GiB and 16.936916 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m WARNING 09-02 18:14:33 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/colligo/anaconda3/envs/steermoe-env/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_A100-SXM4-80GB.json']\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:38 [gpu_worker.py:276] Available KV cache memory: 17.83 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:39 [kv_cache_utils.py:849] GPU KV cache size: 194,784 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:39 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 47.55x\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:39 [core.py:214] init engine (profile, create kv cache, warmup model) took 6.64 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m INFO 09-02 18:14:46 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "INFO 09-02 18:14:46 [llm.py:298] Supported_tasks: ('generate',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m WARNING 09-02 18:14:55 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n",
      "\u001b[1;36m(EngineCore_0 pid=3338166)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 09-02 18:18:06 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "register_vllm_models()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL, \n",
    "    max_seq_len_to_capture=4096, max_model_len=4096, \n",
    "    tensor_parallel_size=torch.cuda.device_count(), gpu_memory_utilization=0.95, max_num_seqs=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d36f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Count to fifteen.\",\n",
    "        }\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34158f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX EXPERTS: 1175 949\n",
      "##### Total Experts: 6144, Layers: 48, Experts: 128\n",
      "##### Num Experts: 0, Steering Magnitude: 1000, Reverse Effect: 0, pos_num_experts: 0, neg_num_experts: 0, metric=risk_diff, strategy: risk_diff\n",
      "\n",
      "INFO 09-02 18:14:55 [chat_utils.py:470] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51c004497364387a4506931bf896477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30dbe49d3224546ae57af37ccd876a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Before Steering:\n",
      "[\"Sure! Here's the count from one to fifteen:\\n\\n1. One  \\n2. Two  \\n3. Three  \\n4. Four  \\n5. Five  \\n6. Six  \\n7. Seven  \\n8. Eight  \\n9. Nine  \\n10. Ten  \\n11. Eleven  \\n12. Twelve  \\n13\"]\n"
     ]
    }
   ],
   "source": [
    "### Before Steering\n",
    "paired_ttest_df = steer_moe(\n",
    "    llm, config[\"activations_path\"],\n",
    "    num_pos_experts=0, num_neg_experts=0,\n",
    "    steering_magnitude=1000, reverse_effect=0, strategy=\"risk_diff\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1, top_k=1, min_p=0, max_tokens=config[\"max_tokens\"], seed=0)\n",
    "outputs = llm.chat(batch_messages, sampling_params, use_tqdm=True, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"},)\n",
    "generations = [output.outputs[0].text for output in outputs]\n",
    "print(\"### Before Steering:\")\n",
    "print(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc67006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX EXPERTS: 1175 949\n",
      "##### Total Experts: 6144, Layers: 48, Experts: 128\n",
      "##### Num Experts: 250, Steering Magnitude: 1000, Reverse Effect: 0, pos_num_experts: 0, neg_num_experts: 250, metric=risk_diff, strategy: risk_diff\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330ac30e95e4dacab83cc28127486ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bc192e05fe4b0ba2acb5983184e26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### After Steering Towards Digits:\n",
      "['1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15.']\n"
     ]
    }
   ],
   "source": [
    "### After Steering Towards Digits\n",
    "paired_ttest_df = steer_moe(\n",
    "    llm, config[\"activations_path\"],\n",
    "    num_pos_experts=config[\"num_pos_experts\"], num_neg_experts=config[\"num_neg_experts\"],\n",
    "    steering_magnitude=1000, reverse_effect=0, strategy=\"risk_diff\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1, top_k=1, min_p=0, max_tokens=config[\"max_tokens\"], seed=0)\n",
    "outputs = llm.chat(batch_messages, sampling_params, use_tqdm=True, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"},)\n",
    "generations = [output.outputs[0].text for output in outputs]\n",
    "print(\"### After Steering Towards Digits:\")\n",
    "print(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebaf694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX EXPERTS: 949 1175\n",
      "##### Total Experts: 6144, Layers: 48, Experts: 128\n",
      "##### Num Experts: 250, Steering Magnitude: 1000, Reverse Effect: 1, pos_num_experts: 0, neg_num_experts: 250, metric=risk_diff, strategy: risk_diff\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db6438561e14567aa75c51a982fc50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cd2919dde44c3488df6cea76a8b971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### After Steering Away from Digits:\n",
      "['One, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen.']\n"
     ]
    }
   ],
   "source": [
    "### After Steering Away from Digits\n",
    "paired_ttest_df = steer_moe(\n",
    "    llm, config[\"activations_path\"],\n",
    "    num_pos_experts=config[\"num_pos_experts\"], num_neg_experts=config[\"num_neg_experts\"],\n",
    "    steering_magnitude=1000, reverse_effect=1, strategy=\"risk_diff\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1, top_k=1, min_p=0, max_tokens=config[\"max_tokens\"], seed=0)\n",
    "outputs = llm.chat(batch_messages, sampling_params, use_tqdm=True, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"},)\n",
    "generations = [output.outputs[0].text for output in outputs]\n",
    "print(\"### After Steering Away from Digits:\")\n",
    "print(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdc143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steermoe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
